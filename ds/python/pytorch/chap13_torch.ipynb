{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sonar3.csv', header=None)\n",
    "X=df.iloc[:, :-1]\n",
    "y=df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(df):\n",
    "    return torch.from_numpy(df.values).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_to_tensor(X_train)\n",
    "X_test=df_to_tensor(X_test)\n",
    "y_train=df_to_tensor(y_train)\n",
    "y_test=df_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mineral(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mineral, self).__init__()\n",
    "        self.hidden_linear=nn.Linear(60, 24)\n",
    "        self.hidden_activation=nn.ReLU()\n",
    "        self.hidden_linear2=nn.Linear(24, 10)\n",
    "        self.hidden_activation2=nn.ReLU()\n",
    "        self.output_linear=nn.Linear(10,1)\n",
    "        self.output_activation=nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.hidden_linear(x)\n",
    "        x=self.hidden_activation(x)\n",
    "        x=self.hidden_linear2(x)\n",
    "        x=self.hidden_activation2(x)\n",
    "        x=self.output_linear(x)\n",
    "        x=self.output_activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mineral(\n",
       "  (hidden_linear): Linear(in_features=60, out_features=24, bias=True)\n",
       "  (hidden_activation): ReLU()\n",
       "  (hidden_linear2): Linear(in_features=24, out_features=10, bias=True)\n",
       "  (hidden_activation2): ReLU()\n",
       "  (output_linear): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (output_activation): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Mineral().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_linear.weight : 60 x 24 = 1440\n",
      "hidden_linear.bias : 24\n",
      "----------------------------------------\n",
      "hidden_linear2.weight : 24 x 10 = 240\n",
      "hidden_linear2.bias : 10\n",
      "----------------------------------------\n",
      "output_linear.weight : 10 x 1 = 10\n",
      "output_linear.bias : 1\n",
      "----------------------------------------\n",
      "total: 1725\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_param = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_param = np.prod(param.size())\n",
    "            if param.dim() > 1:\n",
    "                print(name, ':', ' x '.join(str(x) for x in list(param.size())[::-1]), '=', num_param)\n",
    "            else:\n",
    "                print(name, ':', num_param)\n",
    "                print('-' * 40)\n",
    "            total_param += num_param\n",
    "    print('total:', total_param)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6852\n",
      "Epoch: 1, Loss: 0.6857\n",
      "Epoch: 2, Loss: 0.6745\n",
      "Epoch: 3, Loss: 0.6660\n",
      "Epoch: 4, Loss: 0.6549\n",
      "Epoch: 5, Loss: 0.6382\n",
      "Epoch: 6, Loss: 0.6158\n",
      "Epoch: 7, Loss: 0.5775\n",
      "Epoch: 8, Loss: 0.5431\n",
      "Epoch: 9, Loss: 0.5030\n",
      "Epoch: 10, Loss: 0.4577\n",
      "Epoch: 11, Loss: 0.4136\n",
      "Epoch: 12, Loss: 0.3744\n",
      "Epoch: 13, Loss: 0.3380\n",
      "Epoch: 14, Loss: 0.3065\n",
      "Epoch: 15, Loss: 0.2799\n",
      "Epoch: 16, Loss: 0.2572\n",
      "Epoch: 17, Loss: 0.2405\n",
      "Epoch: 18, Loss: 0.2242\n",
      "Epoch: 19, Loss: 0.2127\n",
      "Epoch: 20, Loss: 0.2022\n",
      "Epoch: 21, Loss: 0.1935\n",
      "Epoch: 22, Loss: 0.1851\n",
      "Epoch: 23, Loss: 0.1794\n",
      "Epoch: 24, Loss: 0.1725\n",
      "Epoch: 25, Loss: 0.1667\n",
      "Epoch: 26, Loss: 0.1615\n",
      "Epoch: 27, Loss: 0.1566\n",
      "Epoch: 28, Loss: 0.1521\n",
      "Epoch: 29, Loss: 0.1483\n",
      "Epoch: 30, Loss: 0.1432\n",
      "Epoch: 31, Loss: 0.1389\n",
      "Epoch: 32, Loss: 0.1337\n",
      "Epoch: 33, Loss: 0.1302\n",
      "Epoch: 34, Loss: 0.1248\n",
      "Epoch: 35, Loss: 0.1211\n",
      "Epoch: 36, Loss: 0.1172\n",
      "Epoch: 37, Loss: 0.1146\n",
      "Epoch: 38, Loss: 0.1099\n",
      "Epoch: 39, Loss: 0.1087\n",
      "Epoch: 40, Loss: 0.1048\n",
      "Epoch: 41, Loss: 0.1012\n",
      "Epoch: 42, Loss: 0.0994\n",
      "Epoch: 43, Loss: 0.0966\n",
      "Epoch: 44, Loss: 0.0935\n",
      "Epoch: 45, Loss: 0.0901\n",
      "Epoch: 46, Loss: 0.0885\n",
      "Epoch: 47, Loss: 0.0855\n",
      "Epoch: 48, Loss: 0.0848\n",
      "Epoch: 49, Loss: 0.0815\n",
      "Epoch: 50, Loss: 0.0794\n",
      "Epoch: 51, Loss: 0.0760\n",
      "Epoch: 52, Loss: 0.0746\n",
      "Epoch: 53, Loss: 0.0718\n",
      "Epoch: 54, Loss: 0.0707\n",
      "Epoch: 55, Loss: 0.0688\n",
      "Epoch: 56, Loss: 0.0665\n",
      "Epoch: 57, Loss: 0.0639\n",
      "Epoch: 58, Loss: 0.0620\n",
      "Epoch: 59, Loss: 0.0608\n",
      "Epoch: 60, Loss: 0.0589\n",
      "Epoch: 61, Loss: 0.0571\n",
      "Epoch: 62, Loss: 0.0566\n",
      "Epoch: 63, Loss: 0.0527\n",
      "Epoch: 64, Loss: 0.0532\n",
      "Epoch: 65, Loss: 0.0508\n",
      "Epoch: 66, Loss: 0.0500\n",
      "Epoch: 67, Loss: 0.0486\n",
      "Epoch: 68, Loss: 0.0463\n",
      "Epoch: 69, Loss: 0.0437\n",
      "Epoch: 70, Loss: 0.0442\n",
      "Epoch: 71, Loss: 0.0446\n",
      "Epoch: 72, Loss: 0.0400\n",
      "Epoch: 73, Loss: 0.0403\n",
      "Epoch: 74, Loss: 0.0408\n",
      "Epoch: 75, Loss: 0.0379\n",
      "Epoch: 76, Loss: 0.0366\n",
      "Epoch: 77, Loss: 0.0373\n",
      "Epoch: 78, Loss: 0.0335\n",
      "Epoch: 79, Loss: 0.0337\n",
      "Epoch: 80, Loss: 0.0338\n",
      "Epoch: 81, Loss: 0.0316\n",
      "Epoch: 82, Loss: 0.0309\n",
      "Epoch: 83, Loss: 0.0320\n",
      "Epoch: 84, Loss: 0.0295\n",
      "Epoch: 85, Loss: 0.0293\n",
      "Epoch: 86, Loss: 0.0284\n",
      "Epoch: 87, Loss: 0.0278\n",
      "Epoch: 88, Loss: 0.0264\n",
      "Epoch: 89, Loss: 0.0268\n",
      "Epoch: 90, Loss: 0.0258\n",
      "Epoch: 91, Loss: 0.0258\n",
      "Epoch: 92, Loss: 0.0244\n",
      "Epoch: 93, Loss: 0.0235\n",
      "Epoch: 94, Loss: 0.0233\n",
      "Epoch: 95, Loss: 0.0226\n",
      "Epoch: 96, Loss: 0.0229\n",
      "Epoch: 97, Loss: 0.0222\n",
      "Epoch: 98, Loss: 0.0207\n",
      "Epoch: 99, Loss: 0.0204\n"
     ]
    }
   ],
   "source": [
    "batch_size=10\n",
    "ds=TensorDataset(X_train, y_train)\n",
    "dataloader= DataLoader(ds, batch_size=batch_size)\n",
    "optimizer=optim.Adam(model.parameters())\n",
    "\n",
    "n_epochs=100\n",
    "loss_fn=nn.BCELoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for data, label in dataloader:\n",
    "        out=model(data.to(device))\n",
    "        loss=loss_fn(out, label.unsqueeze(1).to(device)) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() \n",
    "    print(\"Epoch: %d, Loss: %.4f\" %(epoch, float(loss)))\n",
    "    \n",
    "torch.save(model.state_dict(), 'model/mineral.pth') #layer정보는 없고 학습한 파라미터만 뽑아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7143\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model/mineral.pth', map_location=device))\n",
    "test_ds=TensorDataset(X_test, y_test)\n",
    "test_loader=DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "correct=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        pred=model(data.to(device))\n",
    "        result=pred.squeeze(1).ge(torch.tensor(0.5).to(device)) #1차원으로 변경, greater than equal, batch size 만큼의 TF\n",
    "        correct+=result.long().eq(label.to(device)).sum().item() #T=1, F=0\n",
    "        \n",
    "print(\"Accuracy: %.4f\" % (correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
